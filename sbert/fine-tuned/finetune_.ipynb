{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f6d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM\n",
    ")\n",
    "import json, csv\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pickle\n",
    "from tqdm import trange,tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab00ec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('D:\\\\download\\\\比賽\\\\claim_evidence.csv',keep_default_na=False,na_values=[''])\n",
    "data_11620 = pd.read_csv('D:\\\\download\\\\比賽\\\\claim_evidence_train_all.csv',keep_default_na=False,na_values=['']) \n",
    "wiki_data = pd.read_csv('D:\\\\download\\\\比賽\\\\wiki_clean_doc.csv', keep_default_na=False, na_values=[' '], index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ecc8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "149183d3",
   "metadata": {},
   "source": [
    "# 用evidence做fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7afe48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "#for i in tqdm(range(0,20)):\n",
    "for i in tqdm(range(len(data_11620))):\n",
    "    for j in eval(data['text'][i]):\n",
    "        tmp = [data['claim'][i],j]\n",
    "        print(tmp)\n",
    "        train_examples.append(InputExample(texts=tmp, label=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7271317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('hfl/chinese-roberta-wwm-ext-large', device='cuda')\n",
    "#Define your train examples. You need more than just two examples...\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=5000, warmup_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:\\\\Vic\\\\GitHub\\\\Team3068_FactualTextRetrieval\\\\sbert\\\\model\\\\hfl_pretraineds_0511sentBase')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4f731f1",
   "metadata": {},
   "source": [
    "# 用evidence的article做fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_article(item):\n",
    "    tmp = []\n",
    "    for i in item :\n",
    "        if isinstance(i[0], list) == True:\n",
    "            for j in i:\n",
    "                if j[2] == '臺灣海峽危機#第二次臺灣海峽危機（1958）':\n",
    "                    tmp = []\n",
    "                else:\n",
    "                    tmp += [[j[2]]]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4364fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "tmp_ = []\n",
    "train = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    article = check_article(eval(data['evidence'][i]))\n",
    "    if len(article) != 0 :\n",
    "        for j in article:\n",
    "            tmp = [data['claim'][i]]\n",
    "            tmp += [j[0]]\n",
    "            if tmp in tmp_:\n",
    "                continue\n",
    "            else:\n",
    "                #print(tmp)\n",
    "                tmp_ += [tmp]\n",
    "                train_examples.append(InputExample(texts=tmp, label=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64289ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\download\\\\比賽\\\\train_examples.pickle', 'rb') as f:\n",
    "    train_examples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce67d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('hfl/chinese-roberta-wwm-ext-large', device='cuda')\n",
    "#Define your train examples. You need more than just two examples...\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10, warmup_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:\\\\hfl_pretraineds_document_0516')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
