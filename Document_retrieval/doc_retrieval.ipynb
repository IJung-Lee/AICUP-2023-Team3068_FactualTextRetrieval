{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07dba0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import math\n",
    "import operator\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e47c88",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b623dc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki file is exist.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('../Wiki/'):\n",
    "    print('Wiki file is exist.')\n",
    "else:\n",
    "    # 放置Wiki原始資料的資料夾\n",
    "    os.mkdir('../Wiki/')\n",
    "    print('處理Wiki檔案中...')\n",
    "    wiki_sen = utils.wiki_num_sentence('../wiki-pages')\n",
    "    wiki_doc = utils.wiki_doc('../wiki-pages')\n",
    "    wiki_arctext = utils.wiki_arctext_doc('../wiki-pages')\n",
    "    wiki_numtext = utils.wiki_numtext_doc('../wiki-pages')\n",
    "    print('處理完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5bccf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing file is exist.\n"
     ]
    }
   ],
   "source": [
    "def Claim_evidence(DataPath, SavePath):\n",
    "    claim = pd.read_json(path_or_buf=DataPath, lines=True)\n",
    "    wiki = pd.read_csv('../Wiki/wiki_clean.csv', keep_default_na=False, na_values=[' '])\n",
    "    func = lambda x:[y for l in x for y in func(l)] if type(x[0]) == list else [x]\n",
    "    text = []\n",
    "    for i in tqdm(claim['evidence']):\n",
    "        evs = func(i)\n",
    "        txs = []\n",
    "        for ev in evs:\n",
    "            try:\n",
    "                txs.append(wiki[(wiki['Article']==ev[2]) & (wiki['Num'] == ev[3])].Text.tolist()[0])\n",
    "            except: pass\n",
    "        text.append(txs)\n",
    "    claim['text'] = text\n",
    "    tem = []\n",
    "    for s in claim['claim']:\n",
    "        tem.append(utils.clean_space(s))\n",
    "    claim['claim'] = tem\n",
    "    claim.to_csv(SavePath, index=False)\n",
    "    return claim\n",
    "\n",
    "if os.path.exists('../data_preprocessing/'):\n",
    "    print('data preprocessing file is exist.')\n",
    "else:\n",
    "    os.mkdir('../data_preprocessing/')\n",
    "    print('處理訓練檔案中...')\n",
    "    claim1 = Claim_evidence('./Data/public_train_0316.jsonl', '../data_preprocessing/claim_evidence_train1.csv')\n",
    "    claim2 = Claim_evidence('./Data/public_train_0522.jsonl', '../data_preprocessing/claim_evidence_train2.csv')\n",
    "    mix = pd.concat([claim1, claim2], axis=0, ignore_index=True) \n",
    "    mix.to_csv('../data_preprocessing/claim_evidence_train_all.csv', index=False )\n",
    "    print('處理完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b05ca8",
   "metadata": {},
   "source": [
    "## Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77b190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list(seg_list):\n",
    "    cleaned_dict = {}\n",
    "    for i in seg_list:\n",
    "        i = i.strip().lower()\n",
    "        if i != '' and (i not in stop_words):\n",
    "            if i in cleaned_dict:\n",
    "                cleaned_dict[i] = cleaned_dict[i] + 1\n",
    "            else:\n",
    "                cleaned_dict[i] = 1\n",
    "    return cleaned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4f62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_from_db(db, term):\n",
    "    c = db.cursor()\n",
    "    c.execute('SELECT * FROM postings WHERE term=?', (term,))\n",
    "    return(c.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e4685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25F(sentence):\n",
    "    K1 = 1.2\n",
    "    B = 0.75\n",
    "    N = len(wiki)\n",
    "    AVG_L = 130\n",
    "    \n",
    "    words = pseg.cut(sentence)\n",
    "    seg_list = [w.word for w in words if (w.flag.startswith('n') or w.flag.startswith('v'))]\n",
    "    cleaned_dict = clean_list(seg_list)\n",
    "    BM25_scores = {}\n",
    "    for term in cleaned_dict.keys():\n",
    "        r = fetch_from_db(conn, term.lower())\n",
    "        t = fetch_from_db(cont, term.lower())\n",
    "        if (r is None) and (t is None):\n",
    "            continue\n",
    "        if t is None:\n",
    "            title = []\n",
    "        else:\n",
    "            titles = t[2].split('\\n')\n",
    "            df_t = t[1]\n",
    "            idf_t = math.log2((N - df_t + 0.5) / (df_t + 0.5)) #idf\n",
    "            f = idf_t/(df_t + K1)\n",
    "        try:\n",
    "            docs = r[2].split('\\n')\n",
    "            df = r[1]\n",
    "            idf = math.log2((N - df + 0.5) / (df + 0.5)) #idf\n",
    "            for doc in docs:\n",
    "                docid, tf, ld = doc.split('\\t')\n",
    "                docid = int(docid)\n",
    "                tf = int(tf)\n",
    "                ld = int(ld)\n",
    "                if str(docid) in titles:\n",
    "                    s = (K1 * tf * idf) / (tf + K1 * (1 - B + B * ld / AVG_L))\n",
    "                    s = (s * (K1 + f)) / (f * B)\n",
    "                else:\n",
    "                    s = (K1 * tf * idf) / (tf + K1 * (1 - B + B * ld / AVG_L))\n",
    "                if docid in BM25_scores:\n",
    "                    BM25_scores[docid] = BM25_scores[docid] + s\n",
    "                else:\n",
    "                    BM25_scores[docid] = s\n",
    "        except:pass\n",
    "        try:\n",
    "            for title in titles:\n",
    "                title = int(title)\n",
    "                if title not in BM25_scores:\n",
    "                    BM25_scores[title] = f\n",
    "        except:pass\n",
    "    BM25_scores = sorted(BM25_scores.items(), key = operator.itemgetter(1))\n",
    "    BM25_scores.reverse()\n",
    "    if len(BM25_scores) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return BM25_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d735ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(DataPath, SavePath):\n",
    "    train = pd.read_json(path_or_buf=DataPath, lines=True)\n",
    "    claim = list(train['claim'])\n",
    "    docs = []\n",
    "    for sen in tqdm(claim):\n",
    "        docs.append([i[0] for i in BM25F(sen)[0:5]])\n",
    "    df_j = train[['id', 'claim', 'label']]\n",
    "    '''\n",
    "    0:supports\n",
    "    1:refutes\n",
    "    2:NOT ENOUGH INFO\n",
    "    '''\n",
    "    df_j.loc[df_j['label'] == 'supports', 'label' ] = 0\n",
    "    df_j.loc[df_j['label'] == 'refutes', 'label' ] = 1\n",
    "    df_j.loc[df_j['label'] == 'NOT ENOUGH INFO', 'label' ] = 2\n",
    "    evidence = []\n",
    "    for i in docs:\n",
    "        evs = []\n",
    "        for ev in i:\n",
    "            evs.append({ \"Article\":wiki['Article'][ev], \"Text\":eval(wiki['Text'][ev])})\n",
    "        evidence.append(dict(enumerate(evs)))\n",
    "    claim_data = df_j[['id', 'claim', 'label']].to_dict('records')\n",
    "    for i in range(len(claim_data)):\n",
    "        claim_data[i].update({'evidence':evidence[i]})\n",
    "    with open(SavePath, \"w\", encoding='utf-8') as outfile:\n",
    "#         json.dump(claim_data, outfile, indent=4, ensure_ascii=False)\n",
    "        json.dump(claim_data, outfile, ensure_ascii=False)\n",
    "    return claim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab83e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(DataPath, SavePath):\n",
    "    test = pd.read_json(path_or_buf=DataPath, lines=True)\n",
    "    claim = list(test['claim'])\n",
    "    docs = []\n",
    "    for sen in tqdm(claim):\n",
    "        docs.append([i[0] for i in BM25F(sen)[0:5]])\n",
    "    df_j = test\n",
    "    df_j['label'] = 2\n",
    "    evidence = []\n",
    "    for i in docs:\n",
    "        evs = []\n",
    "        if i == []:\n",
    "            evs.append({\"Article\":'None', \"Text\":[]})\n",
    "        else:\n",
    "            for ev in i:\n",
    "                evs.append({\"Article\":wiki['Article'][ev], \"Text\":eval(wiki['Text'][ev])})\n",
    "        evidence.append(dict(enumerate(evs)))\n",
    "    claim_data = df_j[['id', 'claim', 'label']].to_dict('records')\n",
    "    for i in range(len(claim_data)):\n",
    "        claim_data[i].update({'evidence':evidence[i]})\n",
    "    with open(SavePath, \"w\", encoding='utf-8') as outfile:\n",
    "        json.dump(claim_data, outfile, ensure_ascii=False)\n",
    "    return claim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d10a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/46/bn7t4hx56ws0wqtm45j0m6_r0000gn/T/jieba.cache\n",
      "Loading model cost 0.562 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|█████████████████████████████████████████| 989/989 [02:12<00:00,  7.48it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    conn = sqlite3.connect('../ir.db')\n",
    "    cont = sqlite3.connect('../ir_title.db')\n",
    "    \n",
    "    if os.path.exists('./userdict.txt'):\n",
    "        jieba.load_userdict('./userdict.txt')\n",
    "    else:\n",
    "        utils.new_userdict('../Wiki/wiki_clean_doc.csv', './userdict.txt')\n",
    "        jieba.load_userdict('./userdict.txt')\n",
    "    f = open('./stopwords.txt', encoding = 'utf-8')\n",
    "    words = f.read()\n",
    "    stop_words = set(words.split('\\n'))\n",
    "    wiki = pd.read_csv('../Wiki/wiki_clean_numtext_doc.csv', keep_default_na=False, na_values=[' '])\n",
    "    train_0316 = train_data('./Data/public_train_0316.jsonl', './Result/claim_train_BM25F_v3.json')\n",
    "    train_0522 = train_data('./Data/public_train_0522.jsonl', './Result/claim_train2_BM25F_v3.json')\n",
    "    test_public = test_data('./Data/public_test_data.jsonl', './Result/claim_test_BM25F_v3.json')\n",
    "    test_private = test_data('./Data/private_test_data.jsonl', './Result/claim_private_test_BM25F_v3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1853eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
