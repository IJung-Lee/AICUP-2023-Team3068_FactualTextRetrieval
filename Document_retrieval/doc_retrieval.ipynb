{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dba0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import math\n",
    "import operator\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e47c88",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b623dc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理Wiki檔案中...\n",
      "../wiki-pages//.DS_Store\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/46/bn7t4hx56ws0wqtm45j0m6_r0000gn/T/ipykernel_63465/1800014799.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Wiki/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'處理Wiki檔案中...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mwiki_sen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwiki_num_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../wiki-pages/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mwiki_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwiki_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../wiki-pages/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mwiki_arctext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwiki_arctext_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../wiki-pages/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/Team3068_FactualTextRetrieval/Document_retrieval/utils.py\u001b[0m in \u001b[0;36mwiki_num_sentence\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0mdata_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m             )\n\u001b[1;32m   1081\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "if os.path.exists('../Wiki/'):\n",
    "    print('Wiki file is exist.')\n",
    "else:\n",
    "    # 放置Wiki原始資料的資料夾\n",
    "    os.mkdir('../Wiki/')\n",
    "    print('處理Wiki檔案中...')\n",
    "    wiki_sen = utils.wiki_num_sentence('../wiki-pages')\n",
    "    wiki_doc = utils.wiki_doc('../wiki-pages')\n",
    "    wiki_arctext = utils.wiki_arctext_doc('../wiki-pages')\n",
    "    wiki_numtext = utils.wiki_numtext_doc('../wiki-pages')\n",
    "    print('處理完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5bccf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3942/3942 [15:40<00:00,  4.19it/s]\n",
      "100%|███████████████████████████████████████| 7678/7678 [27:25<00:00,  4.67it/s]\n"
     ]
    }
   ],
   "source": [
    "def Claim_evidence(DataPath, SavePath):\n",
    "    claim = pd.read_json(path_or_buf=DataPath, lines=True)\n",
    "    wiki = pd.read_csv('../Wiki/wiki_clean.csv', keep_default_na=False, na_values=[' '])\n",
    "    func = lambda x:[y for l in x for y in func(l)] if type(x[0]) == list else [x]\n",
    "    text = []\n",
    "    for i in tqdm(claim['evidence']):\n",
    "        evs = func(i)\n",
    "        txs = []\n",
    "        for ev in evs:\n",
    "            try:\n",
    "                txs.append(wiki[(wiki['Article']==ev[2]) & (wiki['Num'] == ev[3])].Text.tolist()[0])\n",
    "            except: pass\n",
    "        text.append(txs)\n",
    "    claim['text'] = text\n",
    "    tem = []\n",
    "    for s in claim['claim']:\n",
    "        tem.append(utils.clean_space(s))\n",
    "    claim['claim'] = tem\n",
    "    claim.to_csv(SavePath, index=False)\n",
    "    return claim\n",
    "\n",
    "if os.path.exists('../data_preprocessing/'):\n",
    "    print('data preprocessing file is exist.')\n",
    "else:\n",
    "    os.mkdir('../data_preprocessing/')\n",
    "    print('處理訓練檔案中...')\n",
    "    claim1 = Claim_evidence('./Data/public_train_0316.jsonl', '../data_preprocessing/claim_evidence_train1.csv')\n",
    "    claim2 = Claim_evidence('./Data/public_train_0522.jsonl', '../data_preprocessing/claim_evidence_train2.csv')\n",
    "    mix = pd.concat([claim1, claim2], axis=0, ignore_index=True) \n",
    "    mix.to_csv('../data_preprocessing/claim_evidence_train_all.csv', index=False )\n",
    "    print('處理完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b05ca8",
   "metadata": {},
   "source": [
    "## Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77b190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list(seg_list):\n",
    "    cleaned_dict = {}\n",
    "    for i in seg_list:\n",
    "        i = i.strip().lower()\n",
    "        if i != '' and (i not in stop_words):\n",
    "            if i in cleaned_dict:\n",
    "                cleaned_dict[i] = cleaned_dict[i] + 1\n",
    "            else:\n",
    "                cleaned_dict[i] = 1\n",
    "    return cleaned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4f62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_from_db(db, term):\n",
    "    c = db.cursor()\n",
    "    c.execute('SELECT * FROM postings WHERE term=?', (term,))\n",
    "    return(c.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e4685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25F(sentence):\n",
    "    K1 = 1.2\n",
    "    B = 0.75\n",
    "    N = len(wiki)\n",
    "    AVG_L = 130\n",
    "    \n",
    "    words = pseg.cut(sentence)\n",
    "    seg_list = [w.word for w in words if (w.flag.startswith('n') or w.flag.startswith('v'))]\n",
    "    cleaned_dict = clean_list(seg_list)\n",
    "    BM25_scores = {}\n",
    "    for term in cleaned_dict.keys():\n",
    "        r = fetch_from_db(conn, term.lower())\n",
    "        t = fetch_from_db(cont, term.lower())\n",
    "        if (r is None) and (t is None):\n",
    "            continue\n",
    "        if t is None:\n",
    "            title = []\n",
    "        else:\n",
    "            titles = t[2].split('\\n')\n",
    "            df_t = t[1]\n",
    "            idf_t = math.log2((N - df_t + 0.5) / (df_t + 0.5)) #idf\n",
    "            f = idf_t/(df_t + K1)\n",
    "        try:\n",
    "            docs = r[2].split('\\n')\n",
    "            df = r[1]\n",
    "            idf = math.log2((N - df + 0.5) / (df + 0.5)) #idf\n",
    "            for doc in docs:\n",
    "                docid, tf, ld = doc.split('\\t')\n",
    "                docid = int(docid)\n",
    "                tf = int(tf)\n",
    "                ld = int(ld)\n",
    "                if str(docid) in titles:\n",
    "                    s = (K1 * tf * idf) / (tf + K1 * (1 - B + B * ld / AVG_L))\n",
    "                    s = (s * (K1 + f)) / (f * B)\n",
    "                else:\n",
    "                    s = (K1 * tf * idf) / (tf + K1 * (1 - B + B * ld / AVG_L))\n",
    "                if docid in BM25_scores:\n",
    "                    BM25_scores[docid] = BM25_scores[docid] + s\n",
    "                else:\n",
    "                    BM25_scores[docid] = s\n",
    "        except:pass\n",
    "        try:\n",
    "            for title in titles:\n",
    "                title = int(title)\n",
    "                if title not in BM25_scores:\n",
    "                    BM25_scores[title] = f\n",
    "        except:pass\n",
    "    BM25_scores = sorted(BM25_scores.items(), key = operator.itemgetter(1))\n",
    "    BM25_scores.reverse()\n",
    "    if len(BM25_scores) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return BM25_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d735ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(DataPath, SavePath):\n",
    "    train = pd.read_json(path_or_buf=DataPath, lines=True)\n",
    "    claim = list(train['claim'])\n",
    "    docs = []\n",
    "    for sen in tqdm(claim):\n",
    "        docs.append([i[0] for i in BM25F(sen)[0:5]])\n",
    "    df_j = train[['id', 'claim', 'label']]\n",
    "    '''\n",
    "    0:supports\n",
    "    1:refutes\n",
    "    2:NOT ENOUGH INFO\n",
    "    '''\n",
    "    df_j.loc[df_j['label'] == 'supports', 'label' ] = 0\n",
    "    df_j.loc[df_j['label'] == 'refutes', 'label' ] = 1\n",
    "    df_j.loc[df_j['label'] == 'NOT ENOUGH INFO', 'label' ] = 2\n",
    "    evidence = []\n",
    "    for i in docs:\n",
    "        evs = []\n",
    "        for ev in i:\n",
    "            evs.append({ \"Article\":wiki['Article'][ev], \"Text\":eval(wiki['Text'][ev])})\n",
    "        evidence.append(dict(enumerate(evs)))\n",
    "    claim_data = df_j[['id', 'claim', 'label']].to_dict('records')\n",
    "    for i in range(len(claim_data)):\n",
    "        claim_data[i].update({'evidence':evidence[i]})\n",
    "    with open(SavePath, \"w\", encoding='utf-8') as outfile:\n",
    "#         json.dump(claim_data, outfile, indent=4, ensure_ascii=False)\n",
    "        json.dump(claim_data, outfile, ensure_ascii=False)\n",
    "    return claim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab83e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(DataPath, SavePath):\n",
    "    test = pd.read_json(path_or_buf=DataPath, lines=True)\n",
    "    claim = list(test['claim'])\n",
    "    docs = []\n",
    "    for sen in tqdm(claim):\n",
    "        docs.append([i[0] for i in BM25F(sen)[0:5]])\n",
    "    df_j = test\n",
    "    df_j['label'] = 2\n",
    "    evidence = []\n",
    "    for i in docs:\n",
    "        evs = []\n",
    "        if i == []:\n",
    "            evs.append({\"Article\":'None', \"Text\":[]})\n",
    "        else:\n",
    "            for ev in i:\n",
    "                evs.append({\"Article\":wiki['Article'][ev], \"Text\":eval(wiki['Text'][ev])})\n",
    "        evidence.append(dict(enumerate(evs)))\n",
    "    claim_data = df_j[['id', 'claim', 'label']].to_dict('records')\n",
    "    for i in range(len(claim_data)):\n",
    "        claim_data[i].update({'evidence':evidence[i]})\n",
    "    with open(SavePath, \"w\", encoding='utf-8') as outfile:\n",
    "        json.dump(claim_data, outfile, ensure_ascii=False)\n",
    "    return claim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d10a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/46/bn7t4hx56ws0wqtm45j0m6_r0000gn/T/jieba.cache\n",
      "Loading model cost 0.562 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|█████████████████████████████████████████| 989/989 [02:12<00:00,  7.48it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    conn = sqlite3.connect('../ir.db')\n",
    "    cont = sqlite3.connect('../ir_title.db')\n",
    "    \n",
    "    if os.path.exists('./userdict.txt'):\n",
    "        jieba.load_userdict('./userdict.txt')\n",
    "    else:\n",
    "        utils.new_userdict('../Wiki/wiki_clean_doc.csv', './userdict.txt')\n",
    "        jieba.load_userdict('./userdict.txt')\n",
    "    f = open('./stopwords.txt', encoding = 'utf-8')\n",
    "    words = f.read()\n",
    "    stop_words = set(words.split('\\n'))\n",
    "    wiki = pd.read_csv('../Wiki/wiki_clean_numtext_doc.csv', keep_default_na=False, na_values=[' '])\n",
    "    train_0316 = train_data('./Data/public_train_0316.jsonl', './Result/claim_train_BM25F_v3.json')\n",
    "    train_0522 = train_data('./Data/public_train_0522.jsonl', './Result/claim_train2_BM25F_v3.json')\n",
    "    test_public = test_data('./Data/public_test_data.jsonl', './Result/claim_test_BM25F_v3.json')\n",
    "    test_private = test_data('./Data/private_test_data.jsonl', './Result/claim_private_test_BM25F_v3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1853eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
